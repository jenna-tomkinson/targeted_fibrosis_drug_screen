{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train machine learning models to predict failing or healthy cell status\n",
    "\n",
    "Each model will be trained on an individual plate or all plates from a batch combined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pprint\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from sklearn.base import clone\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import parallel_backend\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from training_utils import downsample_data, get_X_y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed to make sure any random operations performs are reproducible\n",
    "np.random.seed(0)\n",
    "\n",
    "# path to training/testing datasets\n",
    "training_data_path = pathlib.Path(\"./data\")\n",
    "\n",
    "# Find all training datasets\n",
    "training_files = list(training_data_path.glob(\"*_train.parquet\"))\n",
    "\n",
    "# Metadata column used for prediction class\n",
    "label = \"Metadata_cell_type\"\n",
    "\n",
    "# Directory for models to be outputted\n",
    "model_dir = pathlib.Path(\"./models\")\n",
    "model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Directory for label encoder\n",
    "encoder_dir = pathlib.Path(\"./encoder_results\")\n",
    "encoder_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Directory for training indices\n",
    "training_indices_dir = pathlib.Path(\"./training_indices\")\n",
    "training_indices_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all training files as dataframes and store them under 'orig_train_df'\n",
    "training_data_dfs_dict = {\n",
    "    \"_\".join(parts[:2]) if parts[0] == \"combined\" else parts[0]: {\n",
    "        \"orig_train_df\": pd.read_parquet(file)\n",
    "    }\n",
    "    for file in training_files\n",
    "    if (parts := pathlib.Path(file).stem.split(\"_\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform downsampling on training data and output as data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created at training_indices/localhost240927120001_training_data_indices.csv with 536 entries.\n",
      "(536, 684)\n",
      "failing    268\n",
      "healthy    268\n",
      "Name: Metadata_cell_type, dtype: int64\n",
      "CSV file created at training_indices/localhost240928120001_training_data_indices.csv with 572 entries.\n",
      "(572, 641)\n",
      "failing    286\n",
      "healthy    286\n",
      "Name: Metadata_cell_type, dtype: int64\n",
      "CSV file created at training_indices/combined_batch1_training_data_indices.csv with 2408 entries.\n",
      "(2408, 494)\n",
      "failing    1204\n",
      "healthy    1204\n",
      "Name: Metadata_cell_type, dtype: int64\n",
      "CSV file created at training_indices/localhost240926150001_training_data_indices.csv with 878 entries.\n",
      "(878, 657)\n",
      "failing    439\n",
      "healthy    439\n",
      "Name: Metadata_cell_type, dtype: int64\n",
      "CSV file created at training_indices/localhost240927060001_training_data_indices.csv with 422 entries.\n",
      "(422, 652)\n",
      "failing    211\n",
      "healthy    211\n",
      "Name: Metadata_cell_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# for loop to process each dataframe in the dict\n",
    "for plate, info in training_data_dfs_dict.items():\n",
    "    # load in training plate 4 data as downsampled to lowest class\n",
    "    downsample_df = downsample_data(data=info[\"orig_train_df\"], label=label)\n",
    "\n",
    "    # Store the downsampled dataframe under 'downsample_train_df'\n",
    "    training_data_dfs_dict[plate][\"downsample_train_df\"] = downsample_df\n",
    "\n",
    "    # Export sample indices used in training the model to a new one-column CSV file\n",
    "    output_file = f\"{training_indices_dir}/{plate}_training_data_indices.csv\"\n",
    "    pd.DataFrame(downsample_df.index, columns=[\"Index\"]).to_csv(\n",
    "        output_file, index=False\n",
    "    )\n",
    "\n",
    "    print(f\"CSV file created at {output_file} with {len(downsample_df.index)} entries.\")\n",
    "\n",
    "    print(downsample_df.shape)\n",
    "    print(downsample_df[\"Metadata_cell_type\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get X and y data and label encoder for final and shuffled models for all plates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Class Mapping:\n",
      "{'failing': 0, 'healthy': 1}\n"
     ]
    }
   ],
   "source": [
    "# Collect all unique labels across all plates\n",
    "all_labels = set()\n",
    "for plate, info in training_data_dfs_dict.items():\n",
    "    downsample_df = info[\"downsample_train_df\"]\n",
    "    all_labels.update(downsample_df[label].unique())\n",
    "\n",
    "# Fit the LabelEncoder on the combined set of all labels\n",
    "le = LabelEncoder()\n",
    "le.fit(list(all_labels))\n",
    "\n",
    "# Save the label encoder for consistency\n",
    "dump(le, f\"{encoder_dir}/label_encoder_global.joblib\")\n",
    "\n",
    "# Print the global class mapping to verify consistency\n",
    "class_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "print(\"Global Class Mapping:\")\n",
    "print(class_mapping)\n",
    "\n",
    "# Process each plate with the globally consistent encoder\n",
    "for plate, info in training_data_dfs_dict.items():\n",
    "    # Get downsampled dataframe\n",
    "    downsample_df = info[\"downsample_train_df\"]\n",
    "\n",
    "    # Get not shuffled training data from downsampled df (e.g., \"final\")\n",
    "    X_train, y_train = get_X_y_data(df=downsample_df, label=label, shuffle=False)\n",
    "\n",
    "    # Encode the labels for non-shuffled data using the global encoder\n",
    "    y_train_encoded = le.transform(y_train)\n",
    "\n",
    "    # Get shuffled training data from downsampled df (e.g., \"shuffled_baseline\")\n",
    "    X_shuffled_train, y_shuffled_train = get_X_y_data(\n",
    "        df=downsample_df, label=label, shuffle=True\n",
    "    )\n",
    "\n",
    "    # Encode the labels for shuffled data using the global encoder\n",
    "    y_shuffled_train_encoded = le.transform(y_shuffled_train)\n",
    "\n",
    "    # Store the X and y data under respective keys\n",
    "    training_data_dfs_dict[plate][\"X_train\"] = X_train\n",
    "    training_data_dfs_dict[plate][\"y_train\"] = y_train_encoded\n",
    "    training_data_dfs_dict[plate][\"X_shuffled_train\"] = X_shuffled_train\n",
    "    training_data_dfs_dict[plate][\"y_shuffled_train\"] = y_shuffled_train_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the models\n",
    "\n",
    "These hyperparameters are set based on the model training from the [`cellpainting_predicts_cardiac_fibrosis` repository](https://github.com/WayScience/cellpainting_predicts_cardiac_fibrosis). \n",
    "The following model training code is derived from the [model training notebook](https://github.com/WayScience/cellpainting_predicts_cardiac_fibrosis/blob/main/5.machine_learning/0.train_logistic_regression/1.train_models.ipynb).\n",
    "We will be using RandomizedSearchCV to hyperparameterize the model since that is how the original model was trained and we want to remain consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the model and hyper parameter method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set folds for k-fold cross validation (default is 5)\n",
    "straified_k_folds = StratifiedKFold(n_splits=10, shuffle=False)\n",
    "\n",
    "# Set Logistic Regression model parameters (use default for max_iter)\n",
    "logreg_params = {\n",
    "    \"penalty\": \"elasticnet\",\n",
    "    \"solver\": \"saga\",\n",
    "    \"max_iter\": 1000,\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 0,\n",
    "    \"class_weight\": \"balanced\",\n",
    "}\n",
    "\n",
    "# Define the hyperparameter search space for RandomizedSearchCV\n",
    "param_dist = {\n",
    "    \"C\": np.logspace(-3, 3, 7),\n",
    "    \"l1_ratio\": np.linspace(0, 1, 11),\n",
    "}\n",
    "\n",
    "# Set the random search hyperparameterization method parameters (used default for \"cv\" and \"n_iter\" parameter)\n",
    "random_search_params = {\n",
    "    \"param_distributions\": param_dist,\n",
    "    \"scoring\": \"f1_weighted\",\n",
    "    \"random_state\": 0,\n",
    "    \"n_jobs\": -1,\n",
    "    \"cv\": straified_k_folds,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final and shuffled models per plate and combined batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for localhost240927120001 features (final)...\n",
      "Optimal parameters for localhost240927120001 features (final): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/localhost240927120001_final_downsample.joblib\n",
      "Training model for localhost240927120001 features (shuffled)...\n",
      "Optimal parameters for localhost240927120001 features (shuffled): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/localhost240927120001_shuffled_downsample.joblib\n",
      "Training model for localhost240928120001 features (final)...\n",
      "Optimal parameters for localhost240928120001 features (final): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/localhost240928120001_final_downsample.joblib\n",
      "Training model for localhost240928120001 features (shuffled)...\n",
      "Optimal parameters for localhost240928120001 features (shuffled): {'l1_ratio': 0.4, 'C': 100.0}\n",
      "Model saved as: models/localhost240928120001_shuffled_downsample.joblib\n",
      "Training model for combined_batch1 features (final)...\n",
      "Optimal parameters for combined_batch1 features (final): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/combined_batch1_final_downsample.joblib\n",
      "Training model for combined_batch1 features (shuffled)...\n",
      "Optimal parameters for combined_batch1 features (shuffled): {'l1_ratio': 0.4, 'C': 100.0}\n",
      "Model saved as: models/combined_batch1_shuffled_downsample.joblib\n",
      "Training model for localhost240926150001 features (final)...\n",
      "Optimal parameters for localhost240926150001 features (final): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/localhost240926150001_final_downsample.joblib\n",
      "Training model for localhost240926150001 features (shuffled)...\n",
      "Optimal parameters for localhost240926150001 features (shuffled): {'l1_ratio': 0.1, 'C': 1.0}\n",
      "Model saved as: models/localhost240926150001_shuffled_downsample.joblib\n",
      "Training model for localhost240927060001 features (final)...\n",
      "Optimal parameters for localhost240927060001 features (final): {'l1_ratio': 0.4, 'C': 0.1}\n",
      "Model saved as: models/localhost240927060001_final_downsample.joblib\n",
      "Training model for localhost240927060001 features (shuffled)...\n",
      "Optimal parameters for localhost240927060001 features (shuffled): {'l1_ratio': 0.1, 'C': 1.0}\n",
      "Model saved as: models/localhost240927060001_shuffled_downsample.joblib\n"
     ]
    }
   ],
   "source": [
    "# Initialize Logistic Regression and RandomizedSearchCV\n",
    "logreg = LogisticRegression(**logreg_params)\n",
    "random_search = RandomizedSearchCV(logreg, **random_search_params)\n",
    "\n",
    "# Loop through the training data dictionary for both non-shuffled and shuffled data\n",
    "for plate, info in training_data_dfs_dict.items():\n",
    "    # Get the non-shuffled and shuffled data for the current feature type\n",
    "    X_train = info[\"X_train\"]\n",
    "    y_train = info[\"y_train\"]\n",
    "    X_shuffled_train = info[\"X_shuffled_train\"]\n",
    "    y_shuffled_train = info[\"y_shuffled_train\"]\n",
    "    \n",
    "    # Prevent the convergence warning in sklearn, it does not impact the result\n",
    "    with parallel_backend(\"multiprocessing\"):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\", category=ConvergenceWarning, module=\"sklearn\"\n",
    "            )\n",
    "            ########################################################\n",
    "            # Train the model for non-shuffled (final) training data\n",
    "            ########################################################\n",
    "            print(f\"Training model for {plate} features (final)...\")\n",
    "            final_random_search = clone(random_search)\n",
    "            final_random_search.fit(X_train, y_train)\n",
    "            print(\n",
    "                f\"Optimal parameters for {plate} features (final):\",\n",
    "                final_random_search.best_params_,\n",
    "            )\n",
    "\n",
    "            # Save the model for non-shuffled/final data using joblib\n",
    "            final_model_filename = model_dir / f\"{plate}_final_downsample.joblib\"\n",
    "            dump(final_random_search.best_estimator_, final_model_filename)\n",
    "            print(f\"Model saved as: {final_model_filename}\")\n",
    "\n",
    "            ########################################################\n",
    "            # Train the model for shuffled training data\n",
    "            ########################################################\n",
    "            print(f\"Training model for {plate} features (shuffled)...\")\n",
    "            shuffled_random_search = clone(random_search)\n",
    "            shuffled_random_search.fit(X_shuffled_train, y_shuffled_train)\n",
    "            print(\n",
    "                f\"Optimal parameters for {plate} features (shuffled):\",\n",
    "                shuffled_random_search.best_params_,\n",
    "            )\n",
    "\n",
    "            # Save the final model for shuffled data using joblib\n",
    "            shuffled_final_model_filename = (\n",
    "                model_dir / f\"{plate}_shuffled_downsample.joblib\"\n",
    "            )\n",
    "            dump(shuffled_random_search.best_estimator_, shuffled_final_model_filename)\n",
    "            print(f\"Model saved as: {shuffled_final_model_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fibrosis_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
